{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651e2f25",
   "metadata": {},
   "source": [
    "# Classifying MNIST using random fourier features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df027c",
   "metadata": {},
   "source": [
    "We want to solve \n",
    "$$\\min_{\\theta}\\mathbb{E}[y(X)- f(X;\\theta)] + \\lambda\\vert\\vert\\theta\\vert\\vert$$\n",
    "where $X$ is a handwritten digit and $y(X)$ is a one-hot vector containing the correct label. This problem has been approach with many different techniques and perhaps with highest precision when using CNNs. In this notebook we will formulate a \"neural network\" but with complex activation functions such that we are searching for the complex parameters $\\theta \\in \\mathbf{C}^{10\\times K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72ad8c",
   "metadata": {},
   "source": [
    "We formulate the function as \n",
    "\n",
    "$$f_\\theta(x;\\theta) = \\begin{bmatrix}\\sum_{k=1}^K \\theta_i^{(1)}s(\\omega_k \\cdot x) & \\sum_{k=1}^K \\theta_i^{(2)}s(\\omega_k \\cdot x) & \\dots & \\sum_{k=1}^K \\theta_i^{(10)}s(\\omega_k \\cdot x) \\end{bmatrix}^T \\in \\mathbb{C}^{10}$$\n",
    "\n",
    "where $K$ is the number of \"nodes\" in the network and $s(x) = e^{ix}$. If we instead write this using matrix notation we get\n",
    "\n",
    "$$f_\\theta(x;\\theta) = \\begin{bmatrix} \n",
    "\\theta_1^{(1)} & \\theta_2^{(1)} & \\dots & \\theta_K^{(1)} \\\\ \n",
    "\\theta_1^{(2)} & \\theta_2^{(2)} & \\dots & \\theta_K^{(2)} \\\\ \n",
    "\\vdots & \\vdots  &  & \\vdots \\\\ \n",
    "\\theta_1^{(10)} & \\theta_2^{(10)} & \\dots & \\theta_K^{(10)} \\\\ \n",
    "\\end{bmatrix}  \\begin{bmatrix} s(\\omega_1 \\cdot x) \\\\ s(\\omega_2 \\cdot x) \\\\\\vdots \\\\ s(\\omega_K \\cdot x)\n",
    "\\end{bmatrix} = \\Theta S $$\n",
    "\n",
    "Then we have the system we want to solve the least squares problem \n",
    "$$\\min_\\Theta \\vert\\vert y(X) - \\Theta S \\vert\\vert_2^2 + \\lambda \\vert\\vert \\Theta \\vert \\vert$$\n",
    "\n",
    "which has the solution\n",
    "\n",
    "$$\\Theta = Y S^H(SS^H + \\lambda I)^{-1}.$$\n",
    "\n",
    "Let us implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8d33c",
   "metadata": {},
   "source": [
    "Define parameters and the activation function plus a mapping from label to one-hot vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11b3b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3000                 # Number of \"nodes\"\n",
    "sigma = 0.0005           # frequency spread\n",
    "reg_param = 2            # L2-reg\n",
    "J = 50000                # Batch size\n",
    "\n",
    "def s(x):\n",
    "    return np.exp(1j*x)\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d65a6",
   "metadata": {},
   "source": [
    "Import the mnist data set and reshape and convert to one-hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d039ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "N = train_X.shape[0]\n",
    "M = train_X.shape[1]**2\n",
    "train_X = train_X.reshape(N,M)\n",
    "test_X = test_X.reshape(test_X.shape[0],M)\n",
    "train_Y = one_hot(train_y, 10)\n",
    "test_Y = one_hot(test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e4336e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a batch\n",
    "X = train_X[0:J]\n",
    "Y = train_Y[0:J].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c45dcb",
   "metadata": {},
   "source": [
    "Calculate the $S$ matrix from the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63409f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = np.random.normal(0,sigma,(M,K))\n",
    "S = s(np.dot(X, omega)).T\n",
    "S_test = s(np.dot(test_X, omega)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f10fbc",
   "metadata": {},
   "source": [
    "Solve least squares problem with numpy. You could probably do this with some built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84dfbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta =  (Y @ S.conj().T) @ np.linalg.inv(S@ S.conj().T + reg_param*np.identity(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8fb77",
   "metadata": {},
   "source": [
    "Now we can use $\\Theta$ to make predictions by calulating $\\Theta S$ and taking taking $max(Re(\\Theta S))$ as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f5d1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_insample = Theta@S\n",
    "f_outsample = Theta@S_test\n",
    "train_pred = (np.argmax(np.real(f_insample),axis=0) == train_y[0:J]).sum()/J\n",
    "test_pred = (np.argmax(np.real(f_outsample),axis=0) == test_y).sum()/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22d28c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted samples on the training set:  0.9776\n",
      "Correctly predicted samples on the test set:  0.9685\n"
     ]
    }
   ],
   "source": [
    "print('Correctly predicted samples on the training set: ', train_pred)\n",
    "print('Correctly predicted samples on the test set: ', test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fd471",
   "metadata": {},
   "source": [
    "The results are not amazing, but still about 97% out-of-sample accuracy with a linear system which can be solved analytically and easily computed numerically. We could probably improve by finding the optimal frequencies or searching the the optimal distribution of frequencies. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
